{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af18ac-9c4f-4e82-8030-71cf609c2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using the data perform the following operation using python(ETL operation) Load the stock data from the given CSV file in pandas frame\n",
    "Display the first five observation of it\n",
    "Display the last five observation of it\n",
    "Display the first five observations with only two columns\n",
    "List all the attributes name(that is the meta data)\n",
    "Select the any three attributes(Data, close and any other) and move into a new data frame\n",
    "Perform calculation of new variable called gain and add it to new data frames\n",
    "Accordingly take a necessary action\n",
    "Display the time plot of close attribute\n",
    "Create a CVS file of the new data frame \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"BEML.csv\")\n",
    "df\n",
    "df.head()\n",
    "df.tail()\n",
    "df[['Date','Open']].head()\n",
    "df.columns.tolist() #List all the attributes name (metadata)\n",
    "new_df = df[['Date','Close','Last']].copy()\n",
    "df.head() #moving three attributes to new dataframe\n",
    "new_df.head()\n",
    "new_df['Gain'] = df['Open'] - df['Close'] #calculatioon of new variable called gain and add it to new data frames.\n",
    "new_df.head()\n",
    "#Display the time plot of close attribute.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pd.to_datetime(df['Date']), df['Close'])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"BEML Stock Close Price Over Time\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "new_df.to_csv(\"BEML_NEW_DATA.scv\", index = 'True') #Create a CSV file of the new data frame.\n",
    "\n",
    "\"\"\"Select the any two attributes(Data, close ) and move into a new data frame suing drop method\n",
    "Convert time attribure to index\n",
    "Accordingly take a necessary action\n",
    "Display the normal distribution plot of close attribute\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_new = df.drop(columns = ['Open','High','Low','Last','Total Trade Quantity','Turnover (Lacs)'])\n",
    "df_new.head() #Select any two attributes (Data, Close) and move into a new data frame using drop method.\n",
    "df_new['Date'] = pd.to_datetime(df_new['Date']) #Convert time attriibute to index.\n",
    "df_new.set_index('Date',inplace = True)\n",
    "df_new.head()\n",
    "import seaborn as sns #Display the normal distribution plot of close attribute.\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_new['Close'], kde=True)\n",
    "plt.xlabel(\"Close Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Normal Distribution of GLAXO Close Price\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "url = \"https://raw.githubusercontent.com/ayan-zz/Statistics_python/main/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df\n",
    "#Data Preprocessing, Feature Engineering\n",
    "# Drop Column which do not help prediction or add noise. \n",
    "df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "df\n",
    "### Seperate Features & Target\n",
    "X = df.drop('Survived', axis = 1)\n",
    "y = df['Survived']\n",
    "### Identify Columns Type\n",
    "numeric_features = ['Age','Fare','SibSp','Parch']\n",
    "categorical_features = ['Sex','Embarked','Pclass']\n",
    "df['Age']=df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode())\n",
    "print(\"Original Data: \", len(df))\n",
    "#Simple Random Sampling\n",
    "### Random Sampling with 0 state\n",
    "print(\"Simple Random Sampling\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y, train_size = 0.8, random_state = 0)\n",
    "print(\"Train Dataset: \\n\", train['Sex'].value_counts(),\"\\n\")\n",
    "print(\"Test Dataset: \\n\", test['Sex'].value_counts(),\"\\n\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_random_sex = accuracy_score(y_test, y_pred)\n",
    "print(\"Simple Random Sampling Accuracy for SEX Attribute: \", acc_random_sex)\n",
    "### Simple Random Sampling through Embarked\n",
    "print(\"Simple Random Sampling\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 0)\n",
    "print(\"Train Dataset: \\n\", train[\"Embarked\"].value_counts(),\"\\n\")\n",
    "print(\"Test Dataset: \\n\", test['Embarked'].value_counts(), \"\\n\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_random_Embarked = accuracy_score(y_test, y_pred)\n",
    "print(\"Simple Random Sampling Accuracy for EMBARKED Attribute: \", acc_random_Embarked)\n",
    "print(\"Stratified Sampling\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, stratify = df['Sex'], random_state = 42)\n",
    "print(\"Train Dataset: \\n\", train['Sex'].value_counts(),\"\\n\")\n",
    "print(\"Test Dataset: \\n\", test['Sex'].value_counts(),\"\\n\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_stratified_Sex = accuracy_score(y_test, y_pred)\n",
    "print(\"Stratified Sampling accuracy for Sex Attribute: \", acc_stratified_Sex)\n",
    "\n",
    "\"\"\"Q1: Prepare a classification model using Naive Bayes Theorem on breast cancer patient’s data and\n",
    "perform the following\n",
    "a) Separate feature and target variable\n",
    "b) Display the number of samples, number of features and number of outcome present in the\n",
    "target attribute\n",
    "c) Display the data from array form to dataframe\n",
    "d) Perform somebasic data exploration operation on data\n",
    "e) Prepare classification model for the data using Naïve Bayes theorem\n",
    "f)\n",
    "Perform metric preparation of the same(Accuracy, precision, recall confusion matrix etc..)\n",
    "g) Savethemodel in pickle format\n",
    "Q2: Perform an inferencing operation using the created model in another notebook.#Prepare a classification model using Naive Bayes Theorem on breast cancer patient's data\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "data = load_breast_cancer()\n",
    "X = data.data # Separate features and target variable\n",
    "y = data.target\n",
    "features_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "#Display the number of samples, number of features and number of outcome present in the target attribute.\n",
    "print(\"Number of Samples: \", X.shape[0])\n",
    "print(\"Number of Features: \", X.shape[1])\n",
    "print(\"Target Classes: \", np.unique(y))\n",
    "#Display the data from array form to dataframe\n",
    "df = pd.DataFrame(X, columns = data.feature_names)\n",
    "df['target'] = y\n",
    "df.head()\n",
    "df.tail()\n",
    "df.isnull().sum()\n",
    "#Perform some basic data exploration operation on data\n",
    "df.info()\n",
    "df.describe().T[['mean','std','min','max']]\n",
    "df.groupby('target')['mean radius'].mean()\n",
    "df['target'].value_counts()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)\n",
    "#Prepare classification model for the data using Naive Bayes Theorem.\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "#peformance metric preperation\n",
    "#Calculation Predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "y_pred\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the model: \", accuracy)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision of the model: \", precision)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall of the model: \", recall)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred, target_names = target_names))\n",
    "#plot\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "SVC(random_state=0)\n",
    "predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "#Save model in pickle format\n",
    "sample_index = 90\n",
    "X_new = X_test [sample_index]\n",
    "true_label = y_test[sample_index]\n",
    "print(\"True Class: \", target_names[true_label])\n",
    "plt.show()\n",
    "predicted_class = nb_model.predict([X_new])[0]\n",
    "print(\"\\nPredicted Class: \", target_names[predicted_class])\n",
    "import pickle \n",
    "pickle.dump(nb_model, open('model.pkl','wb'))\n",
    "model1 = pickle.load(open('model.pkl','rb'))\n",
    "predicted_class1 = model1.predict([X_new])\n",
    "print(\"\\n Predicted Class: \", target_names[predicted_class1])\n",
    "with open(\"Naive_Bayes_breast_cancer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(nb_model, file)\n",
    "\n",
    "#Perform an inferencing operation using the created model in another notebook.\n",
    "import pickle\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer() #Load Saved Model\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "#Selection one unseen test case\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)\n",
    "sample_index = 90\n",
    "X_new = X_test[sample_index]\n",
    "true_label = y_test[sample_index]\n",
    "print(\"True Class: \", target_names[true_label])\n",
    "#Perform Inference\n",
    "import pickle \n",
    "model1 = pickle.load(open('model.pkl','rb'))\n",
    "predicted_class1 = model1.predict([X_new])\n",
    "print(\"\\n Predicted Class: \", target_names[predicted_class1])\n",
    "\n",
    "Load a dataset which contains 4 variable (2 Numerical and 2 Categorial).\n",
    "df.isnull().sum() #null values\n",
    "Univariate Text Analysis\n",
    "df.columns.to_list()\n",
    "# Calculation for all columns\n",
    "print(\"Basic Calculation on Name using Describe Method: \\n\", df['Name'].describe(), \"\\n\")\n",
    "# Repeat the basic statistical calculation of all four variable using Mean, Median, Mode method.\n",
    "# Calculate Mean on Numerical Variable.\n",
    "print(\"Mean of Score1: \", df['Score1'].mean())\n",
    "print(\"Mean of Score2: \", df['Score2'].mean(),\"\\n\")\n",
    "# Calculate Median on Numerical Variable.\n",
    "print(\"Median of Score1: \", df['Score1'].median())\n",
    "print(\"Median of Score2: \", df['Score2'].median(),\"\\n\")\n",
    "# Calculate Mode on Categorial Variable.\n",
    "print(\"Mode of Name: \", df['Name'].mode())\n",
    "print(\"Mode of City: \", df['City'].mode(),\"\\n\")\n",
    "#Univariate visual analysis of all 4 variables (2 Categorial and 2 Numerical)\n",
    "plt.figure(figsize=(6, 4))#Box Plot of score 1 and score 2\n",
    "sns.boxplot(y=df['Score1'])\n",
    "plt.title(f\"Box Plot of {'Score1'}\")\n",
    "plt.ylabel('Score1')\n",
    "plt.show() \n",
    "#Bivariate Text Analysis\n",
    "#Calculate mean of Score1 by City\n",
    "print(\"\\n Bivariate Text Analysis\")\n",
    "print(\"By City - Score1: \\n\", df.groupby('City')['Score1'].mean(), \"\\n\")\n",
    "#Calculte mean of Score2 by Name\n",
    "print(\"By Name - Score2: \\n\", df.groupby('Name')['Score2'].mean())\n",
    "#Bivariate Visual Analysis - Scatter Plot between two variables Score1 and Score2\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x = df['Score1'], y=df['Score2'])\n",
    "plt.xlabel(\"Score1\")\n",
    "plt.ylabel(\"Score2\")\n",
    "plt.title(\"Scatter Plot between Score1 and Score1\")\n",
    "plt.show()\n",
    "#Draw HeatMap between two variable Score1 and Score2.\n",
    "corr_data = df[['Score1','Score2']].corr()\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(corr_data, annot = True, cmap = 'coolwarm', fmt='.2f')\n",
    "plt.title(\"Heatmap between Score1 and Score2\")\n",
    "plt.show()\n",
    "#Find covariance between two variables.\n",
    "covariMatrix = df[['Score1','Score2']].cov()\n",
    "display(\"Covariance: \", covariMatrix) #in next cell\n",
    "fig, axes = plt.subplots(figsize = (10, 10))\n",
    "sns.scatterplot(data = df, x = 'Score1', y = 'Score2')#in next cell\n",
    "fig = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(pd.crosstab(df['City'], df['Name']), annot = True)\n",
    "#find Co-Relation between two variable\n",
    "correlation = df['Score1'].corr(df['Score2'])\n",
    "cormatrix = df[['Score1','Score2']].corr()\n",
    "display(\"Correlation :\", correlation)\n",
    "display(\"correlation :\", cormatrix)\n",
    "#Write a section of code which handled the outliers present in the data.\n",
    "Q1 = df['Score1'].quantile(0.25)\n",
    "Q3 = df['Score2'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "LowerBoundary = Q1 - (1.5 * IQR)\n",
    "UpperBoundary = Q3 + (1.5 * IQR)\n",
    "display(Q1, Q3, IQR, LowerBoundary, UpperBoundary)\n",
    "outlier = df[(df.Score1 < LowerBoundary) | (df.Score2 > UpperBoundary)]\n",
    "outlier\n",
    "df_no_outlier = df[(df.Score1 >= LowerBoundary) & (df.Score2 <= UpperBoundary)]\n",
    "df_no_outlier\n",
    "\n",
    "\"\"\"1. Write a python code for the given data distribution:\n",
    "Studies show colour blindness affects about 8% of men. A random sample of 10 men is\n",
    "taken.\n",
    "Find the probability that:\n",
    "a. All 10 men are color blind.\n",
    "b. No men are color blind.\n",
    "c. Exactly 2 men are color blind.\n",
    "d. At least 2 men are color blind.\n",
    "2. Write a python code for the given data distribution:\n",
    "The number of calls arriving at a call center follows a Poisson distribution at 10 calls per\n",
    "hour.\n",
    "a. Calculate the probability that the number of calls will be maximum 5.\n",
    "b. Calculate the probability that the number of calls over a 3 hour period will exceed 30\n",
    "calls\"\"\"\n",
    "from scipy.stats import binom\n",
    "n = 10\n",
    "p = 0.08\n",
    "### a.Through Cumulative Distributed Function\n",
    "p_all = binom.cdf(10, n, p)\n",
    "print(f\"All 10 mens are Color blind: {p_all:}\")\n",
    "### Thourgh Probability mass Function\n",
    "p_all = binom.pmf(10, n, p)\n",
    "print(f\"All 10 mens are Color Blind: {p_all}\")\n",
    "###b. Through Cumulative Distributed Function\n",
    "p_none = binom.cdf(0, n, p)\n",
    "print(\"No men are Color Blind: \", p_none)\n",
    "### Through Probability Mass Function\n",
    "p_none = binom.pmf(0, n, p)\n",
    "print(\"No men are Color blind: \", p_none)\n",
    "### c.Though Cumulative Distributed Function\n",
    "p_exact2 = binom.cdf(2, n, p)\n",
    "print(\"Exactly 2 mens are Color Blind: \", p_exact2)\n",
    "### Through Probability Mass Function\n",
    "p_exact2 = binom.pmf(2, n, p)\n",
    "print(\"Exaclty two men are Color Blind: \", p_exact2)\n",
    "#At least 2 men are color blind.\n",
    "p_atleast2 = 1 - binom.cdf(2, n, p)\n",
    "print(\"At least 2 mens are Color Blind\", p_atleast2)\n",
    "### Through Probability Mass Function\n",
    "p_atleast2 = 1 - binom.pmf(2, n, p)\n",
    "print(\"At least 2 mens are color blind: \", p_atleast2)\n",
    "#Q2.\n",
    "call = 10\n",
    "from scipy.stats import poisson\n",
    "p_max = poisson.cdf(5, call)\n",
    "print(\"Maximum 5 Calls: \", p_max)\n",
    "call_3hr = 30\n",
    "p_exceed = 1 - poisson.cdf(30, call_3hr)\n",
    "print(\"Calls Exceed 30 In 3 hours: \", p_exceed)\n",
    "\n",
    "\"\"\"Q. Perform Qualitative Analysis using Normal Distribution\n",
    "Datasets: Glaxo.csv, BEML.csv\n",
    "1. Load the datasets and perform basic descriptive analytics.\n",
    "2. Selection of Attributes: Find out the attributes that are needed for qualitative analysis, and\n",
    "decide which ones to keep.\n",
    "3. Basic Visualization of the selected attribute, to understand its nature.\n",
    "4. With the opinion of experts, calculate a new attribute for performing the qualitative analysis.\n",
    "5. Take necessary actions to remove the anomalies that have happened, due to the addition of\n",
    "the new attribute.\n",
    "6. Make a comparative normal distribution plot of the 'gain' variable for both the stocks and\n",
    "comment on the results (quality of the variable, variance etc.)\n",
    "7. Calculate the exact values of mean and standard deviation of the 'gain' variable for both the\n",
    "sticks data. REpeat the same with the help of interval function.\n",
    "8. Finally, give a conclusion report of the qualitative analysis\"\"\"\n",
    "#Perfrom Qualitative Analysis using Normal Distribution.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_beml = pd.read_csv(\"BEML.csv\")\n",
    "df_beml[0:5]\n",
    "df_glaxo = pd.read_csv(\"GLAXO.csv\")\n",
    "df_glaxo[0:5]\n",
    "df_beml.head()\n",
    "df_beml.isnull().sum()\n",
    "df_beml.describe().T[['mean','std','min','max']]\n",
    "df_beml.columns.to_list()\n",
    "df_glaxo.head()\n",
    "df_glaxo.isnull().sum()\n",
    "df_glaxo.describe().T[['mean','std','min','max']]\n",
    "df_glaxo.columns.to_list()\n",
    "#Selection of Attributes: Find out the attributes that are needed for qualitative analysis, and decide which ones to keep.\n",
    "df_glaxo = df_glaxo[['Date','Close']]\n",
    "df_glaxo.head()\n",
    "df_glaxo.describe().T[['mean','std','min','max']]\n",
    "df_glaxo = df_glaxo.set_index(pd.DatetimeIndex(df_glaxo['Date'])) # Setting the Datetime Index\n",
    "df_beml = df_beml[['Date','Close']]\n",
    "df_beml.head()df_beml.describe().T[['mean','min','max','std']]\n",
    "### Setting Date Time index on BEML\n",
    "df_beml = df_beml.set_index(pd.DatetimeIndex(df_beml['Date']))\n",
    "#3) Basic visualization of the selected attribute, to understand its nature.\n",
    "#### Plot for GLAXO Gain Distribution\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(df_glaxo.Close, label = \"Glaxo\")\n",
    "plt.xlabel(\"Daily Gain\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"GLAXO Daily Returns\")\n",
    "plt.show()\n",
    "#### Plot for BEML Gain Distribution\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(df_beml.Close, label = \"BEML\")\n",
    "plt.xlabel(\"Daily Gain\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"BEML Daily Returns\")\n",
    "plt.show()\n",
    "plt.plot(df_beml.Close)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Close Price of BEML STOCK\")\n",
    "plt.show()\n",
    "### Plot TREND OF CLOSE PRICE OF GLAXO Stock\n",
    "plt.plot(df_glaxo.Close)\n",
    "plt.xlabel(\"TIME\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Clsoe Price of GLAXO STOCK\")\n",
    "plt.show()\n",
    "#With the opinion of experts, calculate a new attribute for performing the qualitative analysis.\n",
    "### Calculating Gain for GLACO STOCK\n",
    "df_glaxo['gain'] = df_glaxo.Close.pct_change(periods = 1)\n",
    "df_glaxo.head()\n",
    "df_glaxo['gain'].describe().T[['mean','std','min','max']]\n",
    "df_glaxo['gain'].info()\n",
    "### Calculating Gain for BEML STOCK\n",
    "df_beml['gain'] = df_beml.Close.pct_change(periods = 1)\n",
    "df_beml.head()\n",
    "df_beml['gain'].describe().T[['mean','std','min','max']]\n",
    "df_beml['gain'].info()\n",
    "sns.distplot(df_glaxo.gain, label = \"GLAXO\")\n",
    "sns.distplot(df_beml.gain, label = \"BEML\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend();\n",
    "### Gain Plot with time\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(df_glaxo.index, df_glaxo.gain)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Gain')\n",
    "plt.title(\"Gain Plot respect to time for GLAXO STOCK\")\n",
    "plt.show()\n",
    "### Gain plot with time on BEML stock\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(df_beml.index, df_beml.gain)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Gain\")\n",
    "plt.title(\"Gain plot respect to time for BEML STOCK\")\n",
    "plt.show()\n",
    "#5) Take necessary actions to remove the anomalies that have happened, due to the addition of the new attribute\n",
    "print(\"Null values in GLAXO: \",df_glaxo.gain.isnull().sum())\n",
    "print(\"Null Values in BEML: \", df_beml.gain.isnull().sum(),\"\\n\")\n",
    "df_glaxo = df_glaxo.dropna()\n",
    "df_beml = df_beml.dropna()\n",
    "print(df_glaxo.head())\n",
    "# Make a comparative normal distribution plot of the 'gain' variable for both the stocks and comment on the results (quality of the variable, variance etc).\n",
    "### Comparative Normal Distribution Plot.\n",
    "sns.kdeplot(df_glaxo.gain, label = \"GLAXO\", color = 'red', fill = True, alpha = 0.3)\n",
    "sns.kdeplot(df_beml.gain, label = \"BEML\", color = 'yellow', fill = True, alpha = 0.3)\n",
    "\n",
    "plt.title(\"Comparative Normal Distribution Plot (GLAXO VS BEML)\")\n",
    "plt.xlabel(\"Daily Gain\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n",
    "plt.show()\n",
    "\n",
    "### Calculate MEAN and STANDARD DEVIATION for GLAXO Dataset Gain\n",
    "glaxo_mean = df_glaxo['gain'].mean()\n",
    "glaxo_std = df_glaxo['gain'].std()\n",
    "print(f\"Glaxo Gain Mean: {glaxo_mean:.6f}, Glaxo Standard Deviation: , {glaxo_std:.6f}\")\n",
    "### Calculate MEAN and STANDARD DEVIATION for BEML Dataset Gain\n",
    "beml_mean = df_beml['gain'].mean()\n",
    "beml_std = df_beml['gain'].std()\n",
    "print(f\" BEML gain Mean: {beml_mean:.6f}, BEML gain Standard Deviation: {beml_std:.6f}\") \n",
    "from scipy import stats\n",
    "### Calculating Mean and Standard Deviation through stats.norm.interval function\n",
    "glaxo_interval = stats.norm.interval(0.95, loc = glaxo_mean, scale = glaxo_std)\n",
    "beml_interval = stats.norm.interval(0.95, loc = beml_mean, scale = beml_std)\n",
    "\n",
    "print(\"GLAXO 95% Interval :\", glaxo_interval)\n",
    "print(\"BEML 95% Interval: \", beml_interval)\n",
    "#8) Finally, give a conclusion report of the qualitative analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bins = [-float('inf'), -0.02, -0.005, 0.005, 0.02, float('inf')]\n",
    "labels = ['High Loss', 'Moderate Loss', 'Stable', 'Moderate Gain', 'High Gain']\n",
    "\n",
    "df_glaxo['performance_type'] = pd.cut(df_glaxo['gain'], bins=bins, labels=labels)\n",
    "df_beml['performance_type'] = pd.cut(df_beml['gain'], bins=bins, labels=labels)\n",
    "\n",
    "# Get counts\n",
    "glaxo_counts = df_glaxo['performance_type'].value_counts().sort_index()\n",
    "beml_counts = df_beml['performance_type'].value_counts().sort_index()\n",
    "\n",
    "# Get percentages\n",
    "glaxo_pct = df_glaxo['performance_type'].value_counts(normalize=True).sort_index() * 100\n",
    "beml_pct = df_beml['performance_type'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"GLAXO Performance Distribution:\")\n",
    "print(glaxo_counts)\n",
    "print(\"\\nBEML Performance Distribution:\")\n",
    "print(beml_counts)\n",
    "\n",
    "print(\"\\nPercentages:\")\n",
    "print(\"GLAXO:\\n\", glaxo_pct)\n",
    "print(\"BEML:\\n\", beml_pct)\n",
    "\n",
    "\n",
    "!pip install factor_analyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "bfi_df = pd.read_csv(\"bfi.csv\")\n",
    "bfi_df.head()\n",
    "trail_cols = [col for col in bfi_df.columns if col in \n",
    "              ['A1', 'A2', 'A3', 'A4', 'A5', \n",
    "               'C1', 'C2', 'C3', 'C4', 'C5',\n",
    "               'E1', 'E2', 'E3', 'E4', 'E5',\n",
    "               'N1', 'N2', 'N3', 'N4', 'N5',\n",
    "               'O1', 'O2', 'O3', 'O4', 'O5']]\n",
    "bfi_df.head()\n",
    "bfi_df.to_string()\n",
    "bfi_df.isnull().sum()\n",
    "#3)Convert Non-Numeric Data into Numeric.\n",
    "X = bfi_df[trail_cols].apply(pd.to_numeric, errors = 'coerce').dropna()\n",
    "X.isnull().sum()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "#Q3  Check the data to have necessary correlation using Bartlett-Sphericity Test.\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X_scaled)\n",
    "print(\"Chi Square Value\", chi_square_value)\n",
    "print(\"Probability Value: \", p_value)\n",
    "#2) Check for the adequecy of sample size using KMO Method (Kaiser-Meyer-Olkim)\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all, kmo_model = calculate_kmo(X_scaled)\n",
    "print(\"KMO All: \", kmo_all)\n",
    "print(\"KMO MODEL: \", kmo_model)\n",
    "#1) Make a scree plot of eigen values of data against number of factors.\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(X_scaled)\n",
    "ev, v = fa.get_eigenvalues()\n",
    "plt.scatter(range(1, X_scaled.shape[1] + 1), ev)\n",
    "plt.plot(range(1, X_scaled.shape[1] + 1), ev)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"EigenValues\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"2) Make a decision regarding the number of factors to be selected based on some conditions. Also define those conditions.\n",
    "Based on the analysis performed, or by the scree plot which is ploted by us.\n",
    "Kaiser's Criterion (EigenValue > 1): Only factors having eigenvalue greater that 1 are retained.\n",
    "From the EigenValue Scree Plot only first five factors have eigenvalue greater than 1.\n",
    "That's by 5 factors selected.\n",
    "In Scree Plot a sharp drop in eigenValues after Factor 5\n",
    "Elbow Point at Factor 5\"\"\"\n",
    "#Using the loading concept, perform Factor Analysis without rotation that is make a subset of data. Example:- Mapping of variables with factors based on the correlation values.\n",
    "fa = FactorAnalyzer(n_factors = 5, rotation = None)\n",
    "fa.fit(X_scaled)\n",
    "loadings = fa.loadings_\n",
    "loadings\n",
    "loadings_df = pd.DataFrame(loadings, index=trail_cols, columns = ['Factor 1','Factor 2','Factor 3','Factor 4','Factor5'])\n",
    "loadings_df\n",
    "#2) Repeat 5.1 using Rotation.\n",
    "fa = FactorAnalyzer(n_factors = 5, rotation = 'varimax')\n",
    "fa.fit(X_scaled)\n",
    "loadings = fa.loadings_\n",
    "loadings\n",
    "loadings_df = pd.DataFrame(loadings, index = trail_cols, columns = ['Factor 1','Factor 2',' Factor 3','Factor 4','Factor 5'])\n",
    "loadings_df\n",
    "#Write a few lines of codes to create a new csv file, which contains our data with respect to the created factors.\n",
    "reduced_df = pd.DataFrame(fa.transform(X_scaled), index = X.index, columns =['Factor 1','Factor 2', 'Factor 3','Factor 4','Factor 5'])\n",
    "reduced_df.head(10)\n",
    "reduced_df.to_csv(\"Factor_based_data.csv\", index = False)\n",
    "#Question-7: Give a better/technical name to the created factors.\n",
    "reduced_df = reduced_df.rename(columns = {\n",
    "    'Factor 1': 'Neuroticism(N)',\n",
    "    'Factor 2': 'Extraversion(E)',\n",
    "    'Factor 3': 'Conscientiousness(C)',\n",
    "    'Factor 4': 'Agreeableness(A)',\n",
    "    'Factor 5': 'Openness to Experience(O)'})\n",
    "reduced_df.to_csv(\"Factor_based_data.csv\", index = False)\n",
    "renamed = pd.read_csv(\"Factor_based_data.csv\")\n",
    "renamed.head()\n",
    "#Question-8: Identify the Latent Variables given in the symbols (in the PPT) and write their names.\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread(\"Image.png\")\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "1st one = Achievement\n",
    "2nd one = Collaboration / Team Work\n",
    "3rd one = Care / HealthCare\n",
    "4th one = Creativity\n",
    "5th one = Motivated / Happiness¶\n",
    "#3) Comment on the results of 5.1 and 5.2\n",
    "Unrotated Factor loadings do not provide a clear or interpretable factor structure.\n",
    "While Varimax rotation significanlty improves factor interpretability and reveals a clear five-factor structure.\n",
    "That's Why\n",
    "Factor 1 = N1 - N5\n",
    "Factor 2 = E1 - E5\n",
    "Factor 3 = C1 - C5\n",
    "Factor 4 = A1 - A5\n",
    "Factor 5 = O1 - O5\n",
    "#1)Identification of necessary attributes (N1 - N5, A1 - A5, C1 - C5, E1 - E5, O1 - O5).\n",
    "Neuroticism (N1 - N5): Measures emotional instability, anxiety, and moodiness.\n",
    "Agreeableness(A1 - A5): Represents cooperation, trust, and empathy toward others.\n",
    "Conscientiousness (C1 - C5): Reflects organization, discipline, and goal-oriented behaviour.\n",
    "Extraversion (E1 - E5): indicates sociability, assertiveness, and energy levels.\n",
    "Openness to Experience (O1 - O5): Captures creativity, imagination, and openness to new ideas.¶\n",
    "\n",
    "\n",
    "Case Study for SWOT Analysis:- Online Food Delivery Platform\n",
    "Business Application (Online Food Ordering and Delivery Services)\n",
    "Strength (Internal Strength)\n",
    "1) Strong Brand recogniton in India\n",
    "2) Large Restaurant Network\n",
    "3) User-friendly mobile application\n",
    "4) Fast Delivery & Real-Time tracking\n",
    "Weaknessess (Internal Challenges)\n",
    "1) High Operational & Delivery Cost\n",
    "2) Dependency on delivery partners\n",
    "3) Low Profit Margins\n",
    "4) Customer Complaints during peak hours.\n",
    "Opportunities\n",
    "1) Growing demand for online food delivery\n",
    "2) Expansion into cloud kitchens\n",
    "3) Subscription servies (Zomato Gold)\n",
    "4) Grocery & Quick Commerce (Blinkit Integration)\n",
    "Threats\n",
    "1) Intense competition from Swiggy\n",
    "2) Rising fuel and labor costs\n",
    "3) Government regulations\n",
    "4) Restaurant partners shifting platforms \n",
    "strengths = {'Strong Brand':(0.4, 5)}\n",
    "weaknesses = {'Low Profit':(0.6, 4)}\n",
    "opportunities = {'Growing Demand': (0.3, 5)}\n",
    "threats = {'Rising Fuel and Labor Cost':(0.1, 4)}\n",
    "print(\"Step-1: Weighted SWOT Analysis\")\n",
    "\n",
    "print(\"\\nStrengths: \")\n",
    "for s, (w, r) in strengths.items():\n",
    "    print(s, '=', w * r)\n",
    "\n",
    "print(\"\\nWeaknesses: \")\n",
    "for w, (wt, rt) in weaknesses.items():\n",
    "    print(w, '=', wt * rt)\n",
    "\n",
    "print(\"\\nOppotunities: \")\n",
    "for o, (w, r) in opportunities.items():\n",
    "    print(o, '=', w * r)\n",
    "\n",
    "print(\"\\nThreat: \")\n",
    "for t, (w, r) in threats.items():\n",
    "    print(t, '=', w * r)\n",
    "for s in strengths:\n",
    "    for w in weaknesses:\n",
    "        print(\"\\n Action:\")\n",
    "        print(\"Use\", s, \"To Expand in\",w)\n",
    "\n",
    "Question-2: Identify a simple Business Application for SWOT Analysis.\n",
    "Case Study for SWOT Analysis:- Local Retail Store (Grocery / Kirana Store)\n",
    "Strength (Internal Strength)\n",
    "1) Locate near residential Area\n",
    "2) Trusted by local customers\n",
    "Weaknessess (Internal Challenges)\n",
    "1) Limited product variety\n",
    "2) No Home Delivery\n",
    "Opportunities\n",
    "1) Start online ordering via WhatsApp\n",
    "2) Partner with delivery apps\n",
    "Threats\n",
    "1) Competiton from supermarkets and online stores\n",
    "\n",
    "#Perform implementation of Step No. 1 of SWOT Analysis, that is gathering information, performing internal and external assessments, followed by analysis.\n",
    "    \n",
    "strengths = {'Trust':(0.6, 5)}\n",
    "weaknesses = {'No Home Delivery': (0.7, 4)}\n",
    "opportunities = {'Start online ordering via WhatsApp': (0.6, 5)}\n",
    "threats = {'Supermarket Competition':(0.5, 4)}\n",
    "\n",
    "print(\"Step-1: Weighted SWOT Analysis\")\n",
    "\n",
    "print(\"\\nStrengths: \")\n",
    "for s, (w, r) in strengths.items():\n",
    "    print(s, '=', w * r)\n",
    "\n",
    "print(\"\\nWeaknesses: \")\n",
    "for w, (wt, rt) in weaknesses.items():\n",
    "    print(w, '=', wt * rt)\n",
    "\n",
    "print(\"\\nOppotunities: \")\n",
    "for o, (w, r) in opportunities.items():\n",
    "    print(o, '=', w * r)\n",
    "\n",
    "print(\"\\nThreat: \")\n",
    "for t, (w, r) in threats.items():\n",
    "    print(t, '=', w * r)\n",
    "\n",
    "#Perform implementation of Step No. 2 of the SWOT analysis, that is using a suitable criteria to convert the analysis into action.       \n",
    "for s in strengths:\n",
    "    for o in opportunities:\n",
    "        print(\"\\nAction:\")\n",
    "        print(\"By Use of\",s,\"They\",o)    \n",
    "\n",
    "#Perfrom Data Preperation, Followed by Model Prediction, followed by Decision Analytics Operations on the given diabetes.csv dataset.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Load the diabetes dataset and explore using text and visual analysis to perfrom identification of attributes.\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes(as_frame = True)\n",
    "df = pd.DataFrame(diabetes.data, columns = diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n",
    "df.head()\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.hist(figsize = (10, 10), bins=20)\n",
    "plt.suptitle(\"Histogram of Diabetes Dataset Features\", fontsize = 16)\n",
    "plt.show()\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=df, orient='h')\n",
    "plt.title(\"Boxplot of Diabetes Features\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), annot = True, cmap='coolwarm', fmt = '.2f')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "# Question-2: Prepare a classification model using different procedures and different sampling techniques. \n",
    "df['Outcome'] = (df['target'] > df['target'].median()).astype(int)\n",
    "df[['target','Outcome']].head()\n",
    "X = df.drop(['target','Outcome'], axis=1)\n",
    "y = df['Outcome']\n",
    "### Sampling Techniques (Random Sampling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.head(5)\n",
    "y_train.head(5)\n",
    "### Sampling Techniques (Stratified Sampling)\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)\n",
    "X_train_ss.head()\n",
    "### Sampling Technique (SMOTE Oversampling)\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train_ss, y_train_ss)\n",
    "y_train_sm.value_counts()\n",
    "#Classification Model\n",
    "#(1) Logistic Regression\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(X_train_sm, y_train_sm)\n",
    "y_pred_lr = lr.predict(X_test_ss)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test_ss, y_pred_lr))\n",
    "cm = confusion_matrix(y_test_ss, y_pred_lr)\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.heatmap(cm, annot=True, fmt = 'd', cmap='Blues',xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels = ['Actual 0','Actual 1']\n",
    "           )\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Acutal Label\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "dt.fit(X_train_sm, y_train_sm)\n",
    "y_pred_dt = dt.predict(X_test_ss)\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(y_test_ss, y_pred_dt))\n",
    "cm = confusion_matrix(y_test_ss, y_pred_dt)\n",
    "print(\"Confusion Matrix - Decision Tree: \\n\", cm)\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'coolwarm', xticklabels = ['Predicted 0 ','Predicted 1'],\n",
    "            yticklabels = ['Actual 0','Actual 1'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree\")\n",
    "plt.show()\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "rf.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_ss)\n",
    "\n",
    "print(\"Random Forest Accuracy: \", accuracy_score(y_test_ss, y_pred_rf))\n",
    "cm = confusion_matrix(y_test_ss, y_pred_rf)\n",
    "print(\"Confusion Matrix - Random Forest: \\n\", cm)\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Greens',xticklabels=['Predicted 0',' Predicted 1'],\n",
    "            yticklabels = ['Actual 0','Actual 1'])\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel('Actual Value')\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()\n",
    "### Question-3 Evaluate the performance of the model by preparing a framework and by calculating the Expected Value. During this calculation, assume the cost matrix(in $) as\n",
    "#### [99   -1]\n",
    "#### [0    45]\n",
    "cost_matrix = np.array([[99, -1],[0,45]])\n",
    "cost_matrix\n",
    "def calculate_expected_value(y_true, y_pred, cost_matrix):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    EV = (\n",
    "    TN * cost_matrix[0, 0] +\n",
    "    FP * cost_matrix[0, 1] +\n",
    "    FN * cost_matrix[1, 0] +\n",
    "    TP * cost_matrix[1, 1])\n",
    "    return EV, cm\n",
    "    \n",
    "accuracy_lr = accuracy_score(y_test_ss, y_pred_lr)\n",
    "EV_lr, cm_lr = calculate_expected_value(y_test_ss, y_pred_lr, cost_matrix)\n",
    "print(\"Accuracy:\", accuracy_lr)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_lr)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_ss, y_pred_lr))\n",
    "print(\"Expected Value ($):\", EV_lr)\n",
    "\n",
    "print(\"===== DECISION TREE =====\")\n",
    "accuracy_dt = accuracy_score(y_test_ss, y_pred_dt)\n",
    "EV_dt, cm_dt = calculate_expected_value(y_test_ss, y_pred_dt, cost_matrix)\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_dt)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_ss, y_pred_dt))\n",
    "print(\"Expected Value ($):\", EV_dt)\n",
    "\n",
    "print(\"===== RANDOM FOREST =====\")\n",
    "accuracy_rf = accuracy_score(y_test_ss, y_pred_rf)\n",
    "EV_rf, cm_rf = calculate_expected_value(y_test_ss, y_pred_rf, cost_matrix)\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_rf)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_ss, y_pred_rf))\n",
    "print(\"Expected Value ($):\", EV_rf)\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\"],\n",
    "    \"Accuracy\": [accuracy_lr, accuracy_dt, accuracy_rf],\n",
    "    \"Expected Value ($)\": [EV_lr, EV_dt, EV_rf]\n",
    "}\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "Question-1: A passport office claims that the passport applications are processed within 30 days of submitting the application form and all necessary documents. The file passport.csv contains processing time of 40 passport applicants. The population standard deviation of the processing time is 12.5 days. Conduct a hypothesis test at significant level = 0.05 to verify the claim made by the passport office.\n",
    "H0 = Application processes within 30 days\n",
    "H1 = Application does not process within 30 days\n",
    "Using Z-Test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "df = pd.read_csv(\"passport.csv\")\n",
    "df.head()\n",
    "import math\n",
    "def z_test(pop_mean, pop_std, sample):\n",
    "    z_score = (sample.mean() - pop_mean) / (pop_std/math.sqrt(len(sample)))\n",
    "    return z_score, stats.norm.cdf(z_score)\n",
    "z_test(30, 12.5, df.processing_time)\n",
    "  \n",
    "H0 = Application processes within 30 days\n",
    "H1 = Application does not process within 30 days\n",
    "but as probability score is greater than sigma (67% > 50%)\n",
    "Result:- Applicaiton processes within 30 Days\n",
    "  \n",
    "Question-2: Aravind Productions (AP) is newly formed movie production house based out of Mumbai, India. AP was interested in understanding the production cost required for producing Bollywood movie. The industry believes that the production house will require INR 500 million on average. It is assumed that the Bollywood movie production cost follows a normal distribution. The production costs of 40 Bolloywood movies in millions of rupees are given in bollywoodmovies.csv file. Conduct and appropriate hypothesis test at 0.05 to check whether the belief about average production cost is corrent One.\n",
    "H0 = Movie made in 500 million (Null Hypothesis)\n",
    "H1 = Movie does not made in 500 million (Alternative Hypothesis)\n",
    "Using One-Sample T-Test\n",
    "  \n",
    "df = pd.read_csv(\"bollywoodmovies.csv\")\n",
    "df.head()\n",
    "t_stat, p_value = stats.ttest_1samp(df.production_cost, 500)\n",
    "print(\"T_statistics: \", t_stat)\n",
    "print(\"Probability Value: \", p_value)\n",
    "  \n",
    "As per the result p_value is 27 percent.\n",
    "Alpha value = 0.05 = 50%\n",
    "Probability value < Alpha Value\n",
    "We have to take H1:- Movie does not made in 500 million\n",
    "\n",
    "Question-3: A company claims that childern who drink their health drink will grow taller that the children who do not drink that health drink. Data in the file healthdrink.xlsx shows average increase in height over one-year period time from two groups: one drinking the health drink and the other not drinking the health drink. At = 0.05 test whether the increase in height for the children who drink the health drink is different that those who do not drink health drink.\n",
    "H0 = Mean Height increase is same (Null Hypothesis)\n",
    "H1 = Mean height increase is different (Alternative Hypothesis)\n",
    "Using Two Sample T-test\n",
    "\n",
    "healthdrink_yes_df = pd.read_excel('healthdrink.xlsx','healthdrink_yes')\n",
    "healthdrink_yes_df.head()\n",
    "\n",
    "heathdrink_no_df = pd.read_excel(\"healthdrink.xlsx\",\"healthdrink_no\")\n",
    "heathdrink_no_df.head()\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(healthdrink_yes_df['height_increase'], heathdrink_no_df['height_increase'])\n",
    "print(\"T_statistics Value: \", t_stat)\n",
    "print(\"Probability Value: \", p_value)\n",
    "\n",
    "Probability value = 1.197\n",
    "Alpha = 0.05 = 50%\n",
    "Here Probability value > Alpha\n",
    "So, Determines if health drink has significant effect\n",
    "           \n",
    "Question-4: The file breakup.csv contains alcohol consumption before and after breakup. Conduct a paired t-test to check whether the alcohol consumption is same after the breakup at 95% confidence interval (or the level of confidence = 0.05)\n",
    "H0 - Mean before = Mean After (Null Hypothesis)\n",
    "H1 - Mean before != Mean After (Alternative Hypothesis)\n",
    "Using Paired T-test\n",
    "\n",
    "df = pd.read_csv(\"breakups.csv\")\n",
    "df.head()\n",
    "sns.distplot(df['Before_Breakup'], label = \"Before_Breakup\", color = 'red')\n",
    "sns.distplot(df['After_Breakup'], label = \"After_Breakup\", color = 'yellow')\n",
    "plt.legend()\n",
    "t_stat, p_value = stats.ttest_rel(df['Before_Breakup'], df['After_Breakup'])\n",
    "print(\"T Statistics Value: \", t_stat)\n",
    "print(\"Probability Value: \", p_value)\n",
    "\n",
    "Probability value = 59%\n",
    "Alpha Value = 0.05 = 50%\n",
    "Here, Probability Value > Alpha Value\n",
    "So, H0:- Mean_Before = Mean_After\n",
    "\n",
    "Question-5: Ms Rachael Khanna the brand manager of ENZO detergent powder at the \"one-stop\" retail was interested in understanding whether the price discounts have any impact on the sales quantily of ENZO. To test whether the proce discounts had any impact, price discount of 0%, 10%, and 20% were given on randomly selected days. The quantity of ENZO sold in a day under different discount levels. Conduct a one way ANOVA to check whether discount had any significant impact on the average sales quantity at 0.05.\n",
    "H0 = Mean sales same for all discount levels\n",
    "H1 = At least one mean differs¶ using anova\n",
    "sns.distplot(df['discount_0'], label = 'No Discount', color = 'red')\n",
    "sns.distplot(df['discount_10'], label = '10% Discount', color = 'yellow')\n",
    "sns.distplot(df['discount_20'], label = \"20% Discount\", color = 'purple')\n",
    "plt.legend()\n",
    "anova_stat, p_value = stats.f_oneway(df['discount_0'], df['discount_10'], df['discount_20'])\n",
    "print(\"ANOVA Statistics Value: \", anova_stat)\n",
    "print(\"Probability Value: \", p_value)\n",
    "  \n",
    "Probability Value = 38%\n",
    "Alpha value = 0.05 = 50%\n",
    "Here, Probability value < Alpha Value\n",
    "So, Discoutns significanlty affect sales\n",
    "\n",
    "perfrom the various Hypothesis Test\n",
    "Question-6: Hanuman Airlines (HA) operated daily flights to several Indian Cities. One of the problems HA faces is the food preferences by the passengers. Captain Cook, the operations manager of HA, believes that 35% of their passengers prefer vegetarian food, 40% prefer non-vegetarian food, 20% low calorie food, and 5% request for diabetic food. A sample of 500 passengers was chosen to analyze the food preferences and the observed frequencies are as follows:\n",
    "Vegetarian: 190\n",
    "Non-vegetarian: 185\n",
    "Low-calorie: 90\n",
    "Diabetic: 35 Conduct a chi-square test to check whether Captain Cook’s belief is true at ∝=0.05.\n",
    "H0 = Captain Cook's belief is True (Null Hypothesis)\n",
    "H1 = Captain Cook's belief is not True (Alternative Hypothesis\n",
    "Using Chi-Square Test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "observation = [190, 185, 90, 35]\n",
    "expected = [0.35 * 500, 0.4 * 500, 0.2 * 500, 0.05 * 500]\n",
    "chi_stat, p_value = stats.chisquare(observation, expected)\n",
    "print(\"Chi_Square Statistics: \", chi_stat)\n",
    "print(\"Probability Value: \",p_value)\n",
    "\n",
    "Probability Value = 59%\n",
    "Alpha Value = 0.05 = 50%\n",
    "Here, Probability Value > Alpha Value\n",
    "So, H0 = Captain Cook's Belief is True (Null Hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dd421-68ff-4dea-b2d3-73b1bd436e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
